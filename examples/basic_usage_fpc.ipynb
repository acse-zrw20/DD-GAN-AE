{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a182c10f",
   "metadata": {},
   "source": [
    "# Autoencoders and POD on flow past cylinder (FPC) dataset\n",
    "\n",
    "Example notebook that displays the functionality of the built package for FPC and reproduces the results from the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a04531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the necessary external package imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Built package imports\n",
    "import ddganAE\n",
    "from ddganAE.utils import calc_pod\n",
    "from ddganAE.models import AAE, AAE_combined_loss, CAE, SVDAE\n",
    "from ddganAE.architectures.cae.D2 import *\n",
    "from ddganAE.architectures.svdae import *\n",
    "from ddganAE.architectures.discriminators import *\n",
    "from ddganAE.preprocessing import convert_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1636fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seeds for reproduceability\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7622d3",
   "metadata": {},
   "source": [
    "## Proper Orthogonal Decomposition (POD)\n",
    "\n",
    "First we try POD to benchmark the other models against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3496c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_grids = np.load(\"./../submodules/DD-GAN/data/processed/snaphsots_field_Velocity_new_4_2000steps.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f74ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data reshaping\n",
    "\n",
    "input_shape = (55, 42, 2)\n",
    "snapshots = convert_2d(snapshots_grids, input_shape, 2000)\n",
    "snapshots = np.array(snapshots).reshape(8000, *input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c62d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSmatrix (8000, 8000)\n"
     ]
    }
   ],
   "source": [
    "# Normalize data and calculate POD\n",
    "\n",
    "# Could also try subtracting the mean but this does not give better results\n",
    "layer = preprocessing.Normalization(axis=None)\n",
    "layer.adapt(snapshots_grids)\n",
    "\n",
    "snapshots_norm = layer(snapshots_grids).numpy()\n",
    "\n",
    "coeffs, R, s = calc_pod(snapshots_norm, nPOD=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f75b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POD MSE loss of the normalized dataset's reconstruction:  0.011914519353747969\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSE\n",
    "mean = 0\n",
    "for j in range(4):\n",
    "    recon = R @ coeffs[j]\n",
    "    for i in range(2000):\n",
    "        mean += tf.keras.losses.MSE(recon[:, i], snapshots_norm[j, :, i]).numpy()/8000\n",
    "\n",
    "print(\"POD MSE loss of the normalized dataset's reconstruction: \", mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78663ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reconstructed grids\n",
    "reconstructed = np.zeros((4, 4620, 2000))\n",
    "\n",
    "for i in range(4):\n",
    "    reconstructed[i, :, :] = R @ coeffs[i]\n",
    "\n",
    "# Undo normalization\n",
    "reconstructed = (reconstructed * np.sqrt(layer.variance.numpy()) + layer.mean.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a31773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to fit in interpolation (legacy) fortran code\n",
    "reconstructed = convert_2d(reconstructed, (55, 42, 2), 2000)\n",
    "reconstructed = np.array(reconstructed).swapaxes(1, 4)\n",
    "\n",
    "# Uncomment line below to save results\n",
    "# np.save(\"reconstruction_pod_10coeffs.npy\", reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca51233c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.contour.QuadContourSet at 0x7f24d26dab50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot a reconstruction to see if it visually corresponds to what we expect\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].contourf(reconstructed[0, 0, :, :, 100])\n",
    "ax[1].contourf(snapshots[100, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3dc64",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b81d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell does preprocessing, it is the same for the CAE and the AAE\n",
    "\n",
    "# Let's load in the data, split and reshape for the autoencoders\n",
    "snapshots_grids = np.load(\"./../submodules/DD-GAN/data/processed/snaphsots_field_Velocity_new_4_2000steps.npy\")\n",
    "\n",
    "# Some data reshaping\n",
    "input_shape = (55, 42, 2)\n",
    "snapshots = convert_2d(snapshots_grids, input_shape, 2000)\n",
    "snapshots = np.array(snapshots).reshape(8000, *input_shape)\n",
    "\n",
    "# Normalize and split dataset\n",
    "layer = preprocessing.Normalization()\n",
    "layer.adapt(snapshots)\n",
    "\n",
    "x_train, x_val = train_test_split(snapshots, test_size=0.1, random_state=seed)\n",
    "x_train = layer(x_train)\n",
    "x_val = layer(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f065e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 55, 42, 32)        1632      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 21, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2688)              14453376  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                26890     \n",
      "=================================================================\n",
      "Total params: 14,574,250\n",
      "Trainable params: 14,574,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 2688)              29568     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5376)              14456064  \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 6, 128)         147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 12, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 24, 32)        18464     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 56, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 56, 48, 2)         578       \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 55, 42, 2)         0         \n",
      "=================================================================\n",
      "Total params: 14,726,050\n",
      "Trainable params: 14,726,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## The hyperparameters set in this cell and the next correspond to the optimal hyperparameters from hyperparameter\n",
    "# optimization\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.00005, beta_1=0.98, beta_2=0.9)\n",
    "\n",
    "# Use this line to create a new model, select any from the list of models provided in the documentation or make\n",
    "# your own\n",
    "encoder, decoder = build_denser_omata_encoder_decoder(input_shape, 10, initializer, info=True, act='elu', dense_act='relu')\n",
    "\n",
    "# Use these lines to load a previously trained model\n",
    "# encoder = tf.keras.models.load_model(\"saved_model_cae/encoder\") \n",
    "# decoder = tf.keras.models.load_model(\"saved_model_cae/decoder\")\n",
    "\n",
    "cae = CAE(encoder, decoder, optimizer)\n",
    "cae.compile(input_shape, pi_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7df253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard logs the results, run `tensorboard --logdir logs` in this directory in a terminal with acces to\n",
    "# Tensorflow. Note that we can extract the MSE for the final report loss directly from tensorboard as the model\n",
    "# evaluates the validation dataset at every epoch\n",
    "cae.train(x_train, 200, val_data=x_val, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28996552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 57ms/step - loss: 0.1182 - accuracy: 0.8882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11818601936101913, 0.8881969451904297]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cae.autoencoder.evaluate(x_val, x_val)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6e6a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment lines below to save the model\n",
    "# !mkdir -p saved_model\n",
    "# cae.encoder.save('saved_model/encoder')\n",
    "# cae.decoder.save('saved_model/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2190a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress all the samples\n",
    "snapshots = layer(snapshots)\n",
    "res = cae.predict(snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffc3f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo normalization\n",
    "res = (res * np.sqrt(layer.variance.numpy()) + layer.mean.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c6a4799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.contour.QuadContourSet at 0x7f230816f550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAD4CAYAAAAeugY9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdbklEQVR4nO2dXawd1XXH/ys29/JhYxtMjcPNNa5BDihqQLIoIXlISfgojQKVoihRhFyJyi9pRZRUCahS1KgvyUs+HvpQq0QglKa0+RCIhxrqEFUVFcEJJDVxHGwcbi+ysdwac03T69qsPtw5Zu65c2b2ntl7z569/z/p6J4zZ87MOues+5t11uyZEVUFIYSQYfOuvgMghBDSHcqcEEISgDInhJAEoMwJISQBKHNCCEmA1SFXtmrtJbp644aQq6xlaups3yFEyZkzQdPCGWd+89oJVb0i9HpXX3qxTm1aH3q1rbl06n/7DsE5b565sO8QvPLbQ0cbczvof+3qjRuw+a/+POQqa5mdOdF3CNExN7+x7xBa8+qfPPBqH+ud2rQe13z9T/tYdStunz3YdwjOeWpue98heOU/Pv7XjbmdbZuFIic5kqLIgXTflw1Zypwir2bIVTlphsJLmyxlTlZCkacNRZ4+lDkhhCRAdjJni4XkBqvyPBjmGLQBc8eVByqn7zl2XeBICCEpQZl7ZpK8Teej5EkXWJXnA2XuAVOB2y6LYieETIIyd4RLgTetg1InJrAqz4vsdoD6IITI+1wfISR+KPOO9CVWl+vlGPP0YFWeH0ZtFhH5DYAFAOcAnFXVHSJyGYDHAFwN4DcAPqmqJ/2EGSeskIcPc5ukgk1l/geqeoOq7igePwBgr6peC2Bv8ZiQIcLcJoOnyw7QuwF8uLj/CIAfA/hSx3gGQwxV+R1XHuDOUD9knds23LvhWaP5Hj15i+dIiKnMFcBTIqIA/lZVdwPYpKpHi+ePAdhU9UIR2QVgFwCsunx9t2g74urozxhE7gr2y9vldjmvL7hiXahYjfDZLzeVd93rKHY/mMr8Q6r6moj8DoCnReRX5SdVVYt/hhUU/xy7AWB660zlPEMiNpGzOu9Mq9wu5/XF17578HndRFuJT1oWhe4eo565qr5W/D0O4IcAbgLwuohsBoDi73FfQbrARVUem8hJd1LIbd+4FHl5mT6WmzONMheRS0Rk7eg+gNsB7AfwBICdxWw7ATzuK8gYiFnkMccWMynmtssWSwjhUujuMGmzbALwQxEZzf/3qvrPIvI8gH8UkfsAvArgk/7C7BfKMlmyz+1JULLDo1HmqvoKgPdXTP8vAB/xEVRMDEHkbXvmue/8zD23J0GRDxMeATpwuPOTuCS0yF3tCE39gs4mUOY1xF6VdxF57lU5WclQRU6W4FkTBwircTJ0KHL3sDIfGC5EzqqcjBOyKnctcrZYlmBlPoHYWiyuqnGKnPSBr0qcIn8Hyjxy2FIhQ8V3K4UiXw5lHiG+BM6qnFThssUSqhdOka+EMo8ISpz0waMnb+kkdAo8DijzHvHZQqHAiQ1lIZuInS2U+KDMA+K7/02Bk6fmtnc+P0vIYYOUtjsoc4+w8ibkHShuv2Qj87n5jVanwd1z7Drj4YnsdROyEso7LNnIvA2hhgVS2sQlLlotbddL+iMrmdtW5z7WT0gKUNzxkZXMQ9KHuKfnpjovY3H2jINISGr0Ie/TR9xcW3XN1lNOltMXpp9DdjL3VZ37lLcLSbdZF8U+XLq2WlzL25WYu6x76FJvIjuZA26E3lXeIQXdlum5KQp9wIyEXCf1lKTdxOkj6wYndJvPM0uZA+2EbivwIQi7CQp9+PhokcQs7TqGVKXbfsbZyhxYKedxueco7yqq3hcFnw9DFXcdsUu9zWeetczHCS3vta9qp9fXsbBFvC0boOBTJ0WBV2H6PkNJv8vnTpm3wFTiPmVtum7fUi9T97lQ9PHjSuBrD/u55s3Ctre9LNeELp9N3YbA5UaTMregSeJ9ynsSa1/VoEKfxOizo9Tjoo1MfMnaZr19it2WUL9yKHMDJkm8i7zXHV6sff7UtunWyx4nFqEDHPoYCzaC6UvedYxiGpLUfUOZT6CuCreReJO0u7zORvgxCX0Eq/XwmEo8RoFXsfbwuyj0Asq8hEkvvEnkbeXdhtG6TKUeo9ABSj0EqUmcrGRwMvfxj99V4iEFXrd+E6nHKnSAI2R8EELiGw52+45Obk9zSG9oBiPz8X9024NZ2g4jjFni46w7vDh4oY8ztIOWfO7sMh0e57Mf3lXcJsuk3NsxCJlPEnG5Snd9wI4PiU/9an7FtDPvnWm1rEmkKnQg/ird96gFl8s3lbgPeZus00bo7JsvEbXMTQUdq8ir5G0yj2vBpwLbMG4wEXkfEifdiFbmfR0aP0nkphI3EbjJMkIIfUjV+SRiqdqHcsRkqiJndR6pzPsQeddq3IXEq5bHKt2MPs+Lc25xVW/rtqFJ5EOUOHmH6MYhDU3kU7+ady7y8eX7JMajVol7chB57sMqjd+9iKwSkRdE5Mni8VYReU5EDonIYyLS2cJDO+ugb9ES/4TI677JQeTErjK/H0D5cvVfA/ANVb0GwEkA93UJJLYeORDf0ENfZF6de83rvslN5DlX50bvXERmAPwRgL8rHguAWwF8r5jlEQD3tAlgem4qSpGT9PGZ17lw0f7X+g6BFJhuxr4J4IsARruLLwfwhqqeLR7PA7iq6oUisktE9onIvnMLb52f3qfEge4iT63FkumG7Ztwkden36qapXdCVOW/fV/lx9MruVbnje9aRD4G4Liq/rTNClR1t6ruUNUdq6fX9C5xwExcMbVYbDccbWPPSegu83rVmkscR9edXIWWMyZDEz8I4OMicheACwFcCuBbANaLyOqiipkBwN9bZEgkm9cUeZ7jzhu/dVV9UFVnVPVqAJ8C8CNV/QyAZwB8ophtJ4DHvUXpEBfVZ2otljK5VOep5bUtse/4jD2+GOmyCf8SgM+LyCEs9RofchOSP0xFFVOLhQRncHldhlX5O+T2WVgdAaqqPwbw4+L+KwBuch8SIWHJMa9Pbp9i9ZsY2Wy6ht4+CNnaGfpnlSu5VaJkOfz2CSFRwl8OdmQhc1aaJHXaVOU5XAQip18r+bxTh/BMhoSQ2IjyFLguybUqN73i0CRSONc5acbFjtBJh/THeHRoyrAyb0kf1XnK49vJMKk7N8tF+1/juVsCkrTM21blp7ZNG1W1bLeQGOjaF27bOzcVNYUehqRl3hUKnRA3tBU6R7SYQ5k3EJvQ2WohPggxsoUVul8oc0ekVqFz5yfxAYXuD8rcgBx76LmOAhoirs4OeHL7lFWF/tv3XcURKxFBmVtgKvSUpE7yw7blQqnHQdIy99EqMB277VPo7JsT37TpoZtIndL3R/IHDflgJPSmU+WOhE75Et8sbHvb+aHrI6HbjiihsPuBMu9AuUqvE3u5Su8qdtOKv8vRnyQtJvXUTeVfrtI5VDBekm6zAOFGZdjsJG3bgmEvntRhuyN0Ydvb1q+x3UnaNzldOi6Lynxhi1iPzihvBGxea9uCAcyqdYqcmDDebjGRWXme2Kr1IW04+iYLmQPvyLlOzJOq+KrpTYI3bcEA7kXtosXCcebDZST0NlVpF7GzBdMv2ch8RFXF3UZcNoK3EXtX2CsngJv2wvgymuReVUV3EXzXqjynFguQoczLuK4+TVoz47LlxaPJUOjajiF+yVrmPjHtubuSOytyEpI2Yg9JblU5QJk7Y3H2DKbnqquQ8V8ANnInxDdrtp4CAJw+sq7V623bMcSO0ffTBGXegsXZ6j5gefoksQPtR8qEgjs/86ROGjai71vuOVblAGVuxSSJm8zromoPAUWeF6ZVX9V8poKvkqsvwecqcoAyN8ZG5CavN5F7aLEPWeRdvx9ij0vBszXTHcrcgDpRzM6cqH3t3PzGxmXGULUPWeSkHaZVeZdltpF7W7HnXJUDlHknmkQ+Pk+T2Ov67IDZgU+2UOKkjttnD56//9TcduvXt9m5OpKyjdRzFzlAmTfi8uf7SOxdqnXAjdQpcVJHWeJ10wAzyZcrdttqvU7qlPg7UOYtManKm147SeqAWbXuo1InZJK0Tea3EbuN1KuETpEvhzKvwbYqv+PKA7XP7zl23bLHfUk9taqcOz/jwUbsNlKnuJvhLuQWVFXlTSKvm2d25kRjpb84e6ZRWgtbpFbUTc+TfPCx83Oc22cPGlX5IWLJgUaZi8iFIvITEfm5iLwkIl8ppm8VkedE5JCIPCYiSZ2EwabaMxG5ybymUm+iLO3RfUp8Jbnmdhd53rvhWdy74VmH0SxBoU/GtB1lUpkvArhVVd8P4AYAd4rIzQC+BuAbqnoNgJMA7msXanz4/tl+x5UHOkndND4KvJGscnvN1lOdRV6+byp20x48hd6NRpnrEqeLhxcUNwVwK4DvFdMfAXCPjwBD0yTKccnaVOXjNL22SejsFXcjp9zuKso6aZtInUL3j1HPXERWiciLAI4DeBrAYQBvqOrZYpZ5AJVXcRWRXSKyT0T2nV08XTVLNPQhx1BVOqmmbW6X8/rc6beCxWtL12rcBpdCp9TtMZK5qp5T1RsAzAC4CcB7TVegqrtVdYeq7lg9vaZdlJ4YVbc+qtw/vvSFytskulbppB1tc7uc16vWXOIzxNa0EWKVcG165K6EDrBKt8VqNIuqvgHgGQAfALBeREZDG2cAvOY2NL+0EaDp2PI6addJnULvj5Ryu+/K1kTobLu4x2Q0yxUisr64fxGA2wAcwFLif6KYbSeAxz3F6JRYes0Uuhu6vOfUchvoJj/bg4Xq4I7R8JhU5psBPCMivwDwPICnVfVJAF8C8HkROQTgcgAP+QszLyj0YCSV27FJz6XQSTONR4Cq6i8A3Fgx/RUs9RgHw5BEd8eVB1YcMVpmduZE7ZGjuTA9N9X6e00pt2MT+Yh7NzyLR0/eUjvPSOh1R4yu2Xqq9ZWQciHoEaA61d85REKK/Idv3rjsNmmernQ5P0xKNJ1tMnV8i7xJxk2Y7kBlld6N4OdmqbtWps919klbcTdV50B1hd7HZ9w3ub3fEa5E7lukJhV6OY6qKp3VeT3Jn5ulb5F3pctBSSRtYm2tTMJmiCOrdHuSlbmPUStVPeqmytkFJgcWkbxwLfKmMxw+evKWFbc2UOj+CN5m8flzuK8qfJLQXVfVJm0Xkj6xVORlodseWNR2Y8BWy2QGX5n7OoLTBXuOXbfs5gK2XfLGp8jbXBZuhG21zp2i7gkqczkz/LP4dRkOSKH7I8aNeY50acGQbvBKQy2Ym9/Yuk+959h1lDFpRYj2yqg6r6qIy5V7U8Vc14Kxkf34rwW2WCZDmbekSeijCr5qHgrdLX1X5aumz3lfR+g+eVPLpU7647BSDwNl3oFyy2XSEZmTpE6hkxSwkTrxy+B3gMZCUy+9r2GNZPjEMnqlji47T4kbBl2Z9/3z2paq1owLoY9vKHI7GrLL+VlcwSFz7qt09svtGKTM+/7H7UKXnadVyyJL5LYBi5mn5ra3FvqkCp8ib2ZQMh+yxMvU7Rw1fW0VFBqJBVOhm7RnKHIzBiPzVERexkbqlDgZGqbDHCdBidsxGJmXhZWa2MdHxYxPq4MiJ7Fju3O0SuKjncAU/GQGOZolZYHNzW80Evn03FTSnwMQzyX++iYngdWJfPw+Wc5gKvNxRiLL6Z89dXmXKX+vo/s5vf+cqNtYVcmbVXo1QWXu40pDVf/gqQjeRl6T3nNKAszxohspYiphVuF29HKloRG+/jFjGHcckrr36kOAdZXyeCymG1uTfSJV88Qkd59jzUfLHbrgXIo85Nj+rp97iDh7bbPE+A8ZEyYiDrHRmrSO8vfXNE/bdZi8Lqf8OX1k3eCF3kTo9xdifSbr6Cr8KHrmlLo9NvJrIzzb5ZNwpFKlV2H7nmyq89g/r677AqKQObHDhTyr2iExSznloaltSa1Kd/lehvy5tG0fDXJoYi643LlbNTrExXL7gL/g3uH0kXVJjOroIt/Ra9dsPXX+NnTavIckK3MbMcVekZbpGudQ3iexp2l8durk9F4nkaTMTRlVeDEKfbz6jC0+Ej/jgu9beJN+QfQdV6zYtluikXnbn85thjpWzRfTQUhsI6ykvBN3/DvKbURLW2wPzgkBRe6OaGTeljoxd1lek9R9VvPjcophAxMDTePpSXvKovcl2PFKkyJ3y+Bl7ou+Kz0O1yQpQoH7I4rRLBTWZFhxDocURpUA6byP3GBl3oGYdpyyLUNShy2aehorcxF5j4g8IyK/FJGXROT+YvplIvK0iLxc/N3gP1wyTg6nwvUFcztuRmPoUxlL7xuTNstZAF9Q1esB3AzgsyJyPYAHAOxV1WsB7C0eW0MRuYefqTFec5v4g3JfSaPMVfWoqv6suL8A4ACAqwDcDeCRYrZHANzjKUbSAgq9GeZ2vFDWS9i0k6x2gIrI1QBuBPAcgE2qerR46hiATRNes0tE9onIvnMLb9msbhDEKk32ze2wze1yXp899T/hAg1E3zJtkhh75isx3gEqImsAfB/A51T1TRE5/5yqqohUXnlCVXcD2A0A01tn3F+dInPGhzBS4va0ye1yXl987buZ1x6gsO0wkrmIXIClZP+Oqv6gmPy6iGxW1aMishnAcV9Bxk4Mh95T4u1gbk8mtbMypk6jzGWpTHkIwAFV/XrpqScA7ATw1eLv414iHCA+Wy+UtjuY282EbLdww9ENk575BwHcC+BWEXmxuN2FpUS/TUReBvDR4jHxTKw9+oHC3I6Ivvv0Q6exMlfVfwMgE57+iNtwCAmHy9ymiEjf8AjQATM7c2LZ47n5jZXTq+YhJGZunz0IAHhqbnvPkQwHyjwh6iQ+Pg+lTobAuNRHj6vIXfyUuQPWvmo3Mm1hi1i/bvSaESbiroNSd8e5xVV9h5AEox2gVcKuk3h5npyFTpl3xFbkbV8zYnH2TGeRl5mdOUGhk4msPbx8jMTCtrcb5zGlvKw6kRMzepX50EdmdJFyG8ZFfseVB87f33PsuqCxkDxpK24TXIg85+qclXlLQop8YYssE3lZ4iPuuPLAeaGPP98kelbnpAqf4i6zZuspVuQOiOLiFEOjD5GPqBJ5+bm65+tw2bohw2fIIs91w9BbZT70Fosvxnd0jqBsyVCp6rMDrMhdwzZLJIwkXnW4fl17hZBYGZd428P1793wLADg0ZO3dI4pZXqROavy5YyLnFU46ZuFbW93arW4GqkyEvnoPoU+GfbMPbHu8KLRfBQ5SY1JbRVbyiKvm1ZFju0byrwFk/raZU5tmw4QCSF5YVqZ5zg8kTLvkfJGYdR6cj1E0Gb8OX8VkDJtKuy619gItqoCZ4ulHsq8JSbVeVs45tse7ofxg6uWyYi2FXPOIjfdcUyZd6CL0H1uDEbwqFASElPx2wo9Z5HbQJl3pI2UTV5TVZ3byJkiJ65Y2PZ2o6htK/gmoT968pbzN2IGZe4AG6HHXJGzvUPqmCR1162YruS48xPgQUPOMDmtrQuRtz3/CiGuiE3eZAnK3DGTpN4kctsLNVPehKwk16ocYJvFGwtbZNmtDSHbHmyxEB/UjcTIWbw+CC5zDiGLjxREbvvLhsSBS6HnvnFgZR4BfYooBZGTYeNCwrmLHKDMo8enbClyEgttZfzU3HaKvIAyHwA+pEuRk9iwFTMlvpzgo1kWZ8+wb96CufmNzs6dQpGTELQ9f3mVpHO+tufpI+uM5mNlPiDm5jd2FjFFToZIriK3gTIfIG2FTJETki7B2yxssSyn7UiWOjGPt2MocRKati0W0p6gMpcz/s9LQihvQlKCp8AdADzQhRDiCsqcEEISoFHmIvJtETkuIvtL0y4TkadF5OXi7wa/YRLiHua2H9gv7weTyvxhAHeOTXsAwF5VvRbA3uIxIUPjYTC3SSI0ylxV/xXAf49NvhvAI8X9RwDc4zYsQvzD3CYp0bZnvklVjxb3jwHY5CgeQvqGuU0GSecdoKqqACZeXkdEdonIPhHZd+6tt7qujpBg1OX2srw+zbwm/dNW5q+LyGYAKP4enzSjqu5W1R2qumPVJZe0XB0hwTDK7WV5vYZ5PYI7P/ujrcyfALCzuL8TwONuwiGkd5jbZJCYDE38LoB/B7BdROZF5D4AXwVwm4i8DOCjxWNCBgVzm6RE4+H8qvrpCU99xHZlOjX5yvWEhMZlbhPSN8GPAOUh7IQQ4h4ezk8IIQnQi8xZnRNCiFtYmRNCSAL0InNeoIIQQtwSXOYUOSGEuIdtFkIISYCgMudl4wghxA9BZc6DhgghxA9ss/QI9x8QQpo4fWSd0XyUOSGEJABl3iM8eIoQ4orGE20R91DihBDXsDIPDEVOCPEBZR4QipwQ4gueApcQ4gzTkRfEPazMCSEkASjzgHBcOSHEF5Q5IYQkAGVOCHEK++busPksKfPAsNVCCPEBLxtHCCEJwMqcEOIctlrCQ5n3AFsthBDXUOY9wDYTIcQ1lDkhxAtstXTD9vOjzAkhJAF4CtweqOuZswVDUuL0kXVYs/VU32FkQS+VOXcAVkORkxRhu8WeNp8Z2yyRQJE3Mz03df5GhgWFbk7bzyp4m4X/iCuhyJsZz5vRY352JBW6bvDYM4+A6bkpIymVhZaLxLjxT4eRrNhDX4mLXy6d2iwicqeIHBSRQyLyQOP8Z6TL6hpZnD2z7DYkmtoHqUit6X2UWyl9vmfb3CbmsOWyxOkj687fXNC6MheRVQD+BsBtAOYBPC8iT6jqL51EZkmVvMvThiLD8TgXZ89UThsio/cx6ZdILN9RbLmdIuMCS7laD7Xx6tJmuQnAIVV9BQBE5B8A3A0gWMLbSG183ljE0cRQ4ixT1c+u63lH+B57z+3ccCG8NhuElH4ldJH5VQD+s/R4HsDvj88kIrsA7CoeLv76y5/f32GdPtgI4ETfQYzBmMwYj2mLo+U25vZ4Xh+5/y9iy2tgGN9ZDAwhpsbc9r4DVFV3A9gNACKyT1V3+F6nDYzJDMa0nNjzGogzLsZkRpuYuuwAfQ3Ae0qPZ4pphAwd5jYZHF1k/jyAa0Vkq4hMAfgUgCfchEVIrzC3yeBo3WZR1bMi8mcA9gBYBeDbqvpSw8t2t12fRxiTGdnE1CK3Y/xsgDjjYkxmWMckquojEEIIIQHhuVkIISQBKHNCCEmAIDKP5dBoEfm2iBwXkf2laZeJyNMi8nLxd0PAeN4jIs+IyC9F5CURub/vmIr1XygiPxGRnxdxfaWYvlVEniu+x8eKnYMh41olIi+IyJMxxFPE0Htux5bXxfqjy+1Y87qIoXNue5d56dDoPwRwPYBPi8j1vtc7gYcB3Dk27QEAe1X1WgB7i8ehOAvgC6p6PYCbAXy2+Gz6jAkAFgHcqqrvB3ADgDtF5GYAXwPwDVW9BsBJAPcFjut+AAdKj3uNJ6Lcfhhx5TUQZ27HmteAi9xWVa83AB8AsKf0+EEAD/peb008VwPYX3p8EMDm4v5mAAd7jO1xLJ0PJKaYLgbwMywdAXkCwOqq7zVAHDNY+ue/FcCTAKTPeKrW2Wdux5zXRQxR5XYseV2s00luh2izVB0afVWA9ZqySVWPFvePAdjURxAicjWAGwE8F0NMxc++FwEcB/A0gMMA3lDVs8Usob/HbwL4IoC3i8eX9xwPEHdu955DI2LK7QjzGnCU29wBWkKXNoPBx2qKyBoA3wfwOVV9M4aYVPWcqt6AparhJgDvDR3DCBH5GIDjqvrTvmIYMn3lEBBfbseU14Db3A5xcYrYD41+XUQ2q+pREdmMpS12METkAiwl+3dU9QcxxFRGVd8QkWew9FNvvYisLiqGkN/jBwF8XETuAnAhgEsBfKvHeEbEnNu951DMuR1JXgMOcztEZR77odFPANhZ3N+Jpd5eEEREADwE4ICqfj2GmIq4rhCR9cX9i7DU6zwA4BkAnwgdl6o+qKozqno1lvLnR6r6mb7iKRFzbvedQ9Hldmx5DTjO7UAN/rsA/BpL/am/DLlzYSyO7wI4CuD/sNSHug9L/am9AF4G8C8ALgsYz4ew9DPzFwBeLG539RlTEdfvAXihiGs/gC8X038XwE8AHALwTwCme/gOPwzgyYji6T23Y8vrIqbocjvmvC7i6JTbPJyfEEISgDtACSEkAShzQghJAMqcEEISgDInhJAEoMwJISQBKHNCCEkAypwQQhLg/wHSc26fKK0M5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot a reconstruction to see if it visually corresponds to what we expect\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].contourf(res[92,:,:,1])\n",
    "ax[1].contourf(snapshots[92, :, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff718a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to how reconstruction legacy code wants it\n",
    "reconstruction = res.reshape((4, 2000, 55, 42, 2)).swapaxes(1, 4)\n",
    "np.save(\"cae_reconstruction.npy\", reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b29c6",
   "metadata": {},
   "source": [
    "## Adversarial Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59034082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell does preprocessing, it is the same for the CAE and the AAE\n",
    "\n",
    "# Let's load in the data, split and reshape for the autoencoders\n",
    "snapshots_grids = np.load(\"./../submodules/DD-GAN/data/processed/snaphsots_field_Velocity_new_4_2000steps.npy\")\n",
    "\n",
    "# Some data reshaping\n",
    "input_shape = (55, 42, 2)\n",
    "snapshots = convert_2d(snapshots_grids, input_shape, 2000)\n",
    "snapshots = np.array(snapshots).reshape(8000, *input_shape)\n",
    "\n",
    "# Normalize and split dataset\n",
    "layer = preprocessing.Normalization()\n",
    "layer.adapt(snapshots)\n",
    "\n",
    "x_train, x_val = train_test_split(snapshots, test_size=0.1, random_state=seed)\n",
    "x_train = layer(x_train)\n",
    "x_val = layer(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64aa7a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 55, 42, 32)        1632      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 21, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9856)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9856)              97150592  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4928)              48575296  \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                49290     \n",
      "=================================================================\n",
      "Total params: 145,795,306\n",
      "Trainable params: 145,795,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 4928)              54208     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 9856)              48580224  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 9856)              97150592  \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 14, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 22, 32)        18464     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 56, 44, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 56, 44, 2)         578       \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 55, 42, 2)         0         \n",
      "=================================================================\n",
      "Total params: 145,840,994\n",
      "Trainable params: 145,840,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 500)               50500     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 52,101\n",
      "Trainable params: 52,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The hyperparameters set in this cell and the next correspond to the optimal hyperparameters from hyperparameter\n",
    "# optimization\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.8, beta_2=0.9)\n",
    "\n",
    "# Use this line to create a new model, select any from the list of models provided in the documentation or make\n",
    "# your own\n",
    "encoder, decoder = build_densest_omata_encoder_decoder(input_shape, 10, initializer, act='elu', dense_act='relu', info=True)\n",
    "discriminator = build_custom_discriminator(10, initializer, info=True)\n",
    "\n",
    "# Use these lines to load a previously trained model\n",
    "# encoder = tf.keras.models.load_model(\"saved_model_aae/encoder\")\n",
    "# decoder = tf.keras.models.load_model(\"saved_model_aae/decoder\")\n",
    "# discriminator = tf.keras.models.load_model(\"saved_model_aae/discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5092aab",
   "metadata": {},
   "source": [
    "### Separate losses\n",
    "\n",
    "Adversarial autoencoder with a separate loss for the discriminator, autoencoder, and generator. Each of these three are trained separately with this method.\n",
    "\n",
    "Execute either the following two cells or the two cells under \"combined losses\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5171ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE(encoder, decoder, discriminator, optimizer)\n",
    "aae.compile(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aae.train(x_train, 200, val_data=x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec005ee",
   "metadata": {},
   "source": [
    "### Combined losses\n",
    "\n",
    "Aversarial autoencoder with a combined (and weighted) loss function for the discriminator and autoencoder, the generator is still trained independently. This model tends to perform significantly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "265eed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE_combined_loss(encoder, decoder, discriminator, optimizer)\n",
    "aae.compile(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d35cf08",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7067915a878a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/master/acse-9/DD-GAN-AE/ddganAE/models/aae.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, epochs, val_data, batch_size, val_batch_size, wandb_log, n_discriminator)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;31m# Calculate the accuracies on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 d_loss_val, g_loss_val = self.validate(val_dataset,\n\u001b[0m\u001b[1;32m    372\u001b[0m                                                        val_batch_size)\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/master/acse-9/DD-GAN-AE/ddganAE/models/aae.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, val_dataset, val_batch_size)\u001b[0m\n\u001b[1;32m    416\u001b[0m                                                  self.latent_dim))\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             d_loss_real = self.discriminator.evaluate(latent_real,\n\u001b[0m\u001b[1;32m    419\u001b[0m                                                       valid, verbose=0)[0]\n\u001b[1;32m    420\u001b[0m             d_loss_fake = self.discriminator.evaluate(latent_fake,\n",
      "\u001b[0;32m~/miniconda3/envs/acse-9/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/acse-9/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/acse-9/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/acse-9/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/acse-9/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/acse-9/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/acse-9/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aae.train(x_train, 200, val_data=x_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31173699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a loss value on the validation set\n",
    "res_val = aae.adversarial_autoencoder.predict(x_val)[0]\n",
    "np.mean(tf.keras.losses.MSE(res_val, x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07addbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = layer(snapshots)\n",
    "res = aae.adversarial_autoencoder.predict(snapshots)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0be908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo normalization\n",
    "res = (res * np.sqrt(layer.variance.numpy()) + layer.mean.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].contourf(res[92,:,:,1])\n",
    "ax[1].contourf(snapshots[92, :, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to fit in interpolation (legacy) fortran code\n",
    "reconstruction = res.reshape((4, 2000, 55, 42, 2)).swapaxes(1, 4)\n",
    "np.save(\"aae_reconstruction.npy\", reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29bd97c",
   "metadata": {},
   "source": [
    "## SVD Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0bcb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Load grids\n",
    "snapshots_grids = np.load(\"./../submodules/DD-GAN/data/processed/snaphsots_field_Velocity_new_4_2000steps.npy\")\n",
    "\n",
    "# Data normalization\n",
    "layer = preprocessing.Normalization(axis=None)\n",
    "layer.adapt(snapshots_grids)\n",
    "\n",
    "snapshots_grids = snapshots_grids.swapaxes(0, 2)\n",
    "\n",
    "x_train, x_val = train_test_split(snapshots_grids, test_size=0.1)\n",
    "x_train = layer(x_train).numpy().swapaxes(0, 2)\n",
    "x_val = layer(x_val).numpy().swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9786213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "optimizer = tf.keras.optimizers.Nadam(lr=0.0005, beta_1=0.98, beta_2=0.99999)\n",
    "\n",
    "encoder, decoder = build_vinicius_encoder_decoder(100, 10, initializer, act='elu', dense_act='relu', info=False, reg=0, dropout=0.55, batchnorm=False)\n",
    "\n",
    "# encoder = tf.keras.models.load_model(\"saved_model_svdae/encoder\")\n",
    "# decoder = tf.keras.models.load_model(\"saved_model_svdae/decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403afee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svdae = SVDAE(encoder, decoder, optimizer)\n",
    "svdae.compile(100, weight_loss=False)\n",
    "\n",
    "# Only set this when loading in the model\n",
    "# svdae.R = np.load(\"R_svdae.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04743d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSmatrix (7200, 7200)\n"
     ]
    }
   ],
   "source": [
    "svdae.train(x_train, 200, val_data=x_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1085d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08573786534303246"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a loss value and create an array of output grids\n",
    "output = np.zeros((4, 2, 55, 42, 200))\n",
    "loss = 0\n",
    "for i in range(x_val.shape[0]):\n",
    "    for j in range(x_val.shape[2]):\n",
    "        original = x_val[i, :, j]\n",
    "        result = svdae.predict_single(original)\n",
    "        loss += tf.keras.losses.MSE(original, result)\n",
    "        result = (result * np.sqrt(layer.variance.numpy()) + layer.mean.numpy())\n",
    "        result = np.expand_dims(result,(0,2))\n",
    "        input_shape = (55, 42, 2)\n",
    "        result = convert_2d(result, input_shape, 1)\n",
    "        output[i, :, :, :, j] = np.moveaxis(np.array(result).reshape(55, 42, 2), 2, 0)\n",
    "        \n",
    "loss.numpy()/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b16cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = x_val[2, :, 30]\n",
    "\n",
    "result = svdae.predict_single(original)\n",
    "result = np.expand_dims(result,(0,2))\n",
    "input_shape = (55, 42, 2)\n",
    "result = convert_2d(result, input_shape, 1)\n",
    "\n",
    "original = np.expand_dims(original,(0,2))\n",
    "original = convert_2d(original, input_shape, 1)\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].contourf(result[0][0, :, :, 0])\n",
    "ax[1].contourf(original[0][0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"svdae_reconstruction.npy\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd77d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "svdae.encoder.save('saved_model_svdae/encoder')\n",
    "svdae.decoder.save('saved_model_svdae/decoder')\n",
    "np.save('saved_model_svdae/R.npy', svdae.R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
